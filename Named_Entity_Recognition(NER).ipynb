{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Format or The Type of the Data is:- <class 'bytes'>\n",
      "Total Length of the Raw Data is :- 120585\n"
     ]
    }
   ],
   "source": [
    "### Import the Text Data from WEB || Backup The Data || Find Format Of The Data\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "Link= 'https://www.gutenberg.org/files/65567/65567-0.txt'\n",
    "RAW_Data = urlopen(url= Link).read()\n",
    "Data_Backup = RAW_Data\n",
    "\n",
    "print('The Format or The Type of the Data is:-',type(RAW_Data))\n",
    "print('Total Length of the Raw Data is :-',len(RAW_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Conversion The Format or The Type of the Data is:- <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "### Converting The Data into String Format\n",
    "\n",
    "RAW_Data= RAW_Data.decode(\"utf-8\")\n",
    "print('After Conversion The Format or The Type of the Data is:-',type(RAW_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\r\\n\\r\\nCONDUCTED BY R. CHAMBERS (SECUNDUS)\\r\\n\\r\\nNO. 17.—VOL. I.       SATURDAY, APRIL 26, 1884.       PRICE 1½_d._]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPOST-OFFICE LIFE-ASSURANCE AND ANNUITIES.\\r\\n\\r\\n\\r\\nThe numerous aids which the government have from time to time afforded\\r\\nthrough the agency of the Post-office for the encouragement of thrift\\r\\nand providence amongst the poorer classes have generally been attended\\r\\nwith so much success, that it is surprising to hear of even one\\r\\nexception in regard to such efforts. There is no doubt, however, as\\r\\nwas pointed out two years ago in this _Journal_, that the existing\\r\\nscheme of Post-'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First few lines of the Data\n",
    "RAW_Data[1400:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find the starting position of Meaningful Data\n",
    "RAW_Data.find('The numerous aids which the government have from time to time afforded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length of the Final Data is :- 118134\n"
     ]
    }
   ],
   "source": [
    "### Extract Meaningful Data from the Raw Data\n",
    "Text_Data= RAW_Data[1568:]\n",
    "print('Total Length of the Final Data is :-',len(Text_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The numerous aids which the government have from time to time afforded\\r\\nthrough the agency of the Post-office for the encouragement of thrift\\r\\nand providence amongst the poorer classes have generally been attended\\r\\nwith so much success, that it is surprising to hear of even one\\r\\nexception in regard to such efforts. There is no doubt, however, as\\r\\nwas pointed out two years ago in this _Journal_, that the existing\\r\\nscheme of Post-office Life-assurance and Annuities, which has been in\\r\\noperation si'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Some sample Data\n",
    "Text_Data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning the data.\n",
    "import re\n",
    "\n",
    "### Removing all the special characters.\n",
    "Text_Data= re.sub(r'[?|.|!|:|,]',r'',Text_Data)\n",
    "\n",
    "### Removing tha [\\r\\n] tag.\n",
    "Text_Data= re.sub(r'[\\r\\n]',r' ',Text_Data)\n",
    "\n",
    "### Removing extra spaces in the text.\n",
    "Text_Data_Cleaned= re.sub(r' +', ' ', Text_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The numerous aids which the government have from time to time afforded through the agency of the Post-office for the encouragement of thrift and providence amongst the poorer classes have generally been attended with so much success that it is surprising to hear of even one exception in regard to such efforts There is no doubt however as was pointed out two years ago in this _Journal_ that the existing scheme of Post-office Life-assurance and Annuities which has been in operation since 1865 has '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Some Sample Data\n",
    "Text_Data_Cleaned[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Total Length of the Data:- 112644\n",
      "Total Number of Words in the Data:- 19799\n"
     ]
    }
   ],
   "source": [
    "print('         Total Length of the Data:-',len(Text_Data_Cleaned))\n",
    "print('Total Number of Words in the Data:-',len(Text_Data_Cleaned.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NER Using Spacy\n",
    "### Installing spacy library and the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installing the spacy library\n",
    "# !pip install spacy\n",
    "\n",
    "### Installing the spacy NER model\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "### Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "FoundEntities= nlp(Text_Data_Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Total Found Named Entity:- 846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Labels_Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Post</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>Numerals that do not fall under another type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two years ago</td>\n",
       "      <td>DATE</td>\n",
       "      <td>Absolute or relative dates or periods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Journal</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post-office</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Project Gutenberg</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>People, including fictional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Project</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Gutenberg Literary Archive Foundation</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>eBooks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>eBooks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Entities    Labels  \\\n",
       "0                                     Post       ORG   \n",
       "1                                      one  CARDINAL   \n",
       "2                            two years ago      DATE   \n",
       "3                                  Journal       ORG   \n",
       "4                              Post-office       ORG   \n",
       "..                                     ...       ...   \n",
       "841                      Project Gutenberg    PERSON   \n",
       "842                                Project       ORG   \n",
       "843  Gutenberg Literary Archive Foundation       ORG   \n",
       "844                                 eBooks       ORG   \n",
       "845                                 eBooks       ORG   \n",
       "\n",
       "                               Labels_Explanation  \n",
       "0         Companies, agencies, institutions, etc.  \n",
       "1    Numerals that do not fall under another type  \n",
       "2           Absolute or relative dates or periods  \n",
       "3         Companies, agencies, institutions, etc.  \n",
       "4         Companies, agencies, institutions, etc.  \n",
       "..                                            ...  \n",
       "841                   People, including fictional  \n",
       "842       Companies, agencies, institutions, etc.  \n",
       "843       Companies, agencies, institutions, etc.  \n",
       "844       Companies, agencies, institutions, etc.  \n",
       "845       Companies, agencies, institutions, etc.  \n",
       "\n",
       "[846 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating Data Frame for Named Entity Tagged Words.\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for ent in FoundEntities.ents:\n",
    "    entities.append(ent.text)\n",
    "    labels.append(ent.label_)\n",
    "    \n",
    "Ner_Data = pd.DataFrame( {'Entities':entities, 'Labels':labels} )\n",
    "Ner_Data['Labels_Explanation']= [spacy.explain(Ner_Data['Labels'][i]) for i in range(len(Ner_Data['Labels']))]\n",
    "\n",
    "print('##### Total Found Named Entity:-',len(Ner_Data))\n",
    "Ner_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NER Using NLTK algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############## After Tokenizing Total Number of POS Tagged words :- 20521\n",
      "############## After Tokenizing Few POS Tagged Words:-  \n",
      "\n",
      " [('The', 'DT'), ('numerous', 'JJ'), ('aids', 'NNS'), ('which', 'WDT'), ('the', 'DT'), ('government', 'NN'), ('have', 'VBP'), ('from', 'IN'), ('time', 'NN'), ('to', 'TO'), ('time', 'NN'), ('afforded', 'VBN'), ('through', 'IN'), ('the', 'DT'), ('agency', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Post-office', 'NNP'), ('for', 'IN'), ('the', 'DT')]\n",
      "\n",
      "############### Few Named Entity Tagged Words ###############\n",
      "\n",
      "(PERSON Annuities/NNP)\n",
      "(PERSON Fawcett/NNP)\n",
      "(ORGANIZATION Committee/NNP)\n",
      "(ORGANIZATION House/NNP)\n",
      "(ORGANIZATION Commons/NNP)\n"
     ]
    }
   ],
   "source": [
    "### Tokenize the Text Data || POS tagging || Labelling Named Entity(NE)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "### Tokenize the Sentence --\n",
    "word_list = word_tokenize(Text_Data)\n",
    "\n",
    "### POS tagging\n",
    "pos_tags = nltk.pos_tag(word_list)\n",
    "print('\\n############## After Tokenizing Total Number of POS Tagged words :-',len(pos_tags))\n",
    "print('############## After Tokenizing Few POS Tagged Words:-  \\n\\n',pos_tags[:20])\n",
    "\n",
    "### Labelling the words with Named Entity(NE)\n",
    "chunk_word=[]\n",
    "chunks = nltk.ne_chunk(pos_tags, binary=False)  # binary=True\n",
    "\n",
    "print('\\n############### Few Named Entity Tagged Words ###############\\n')\n",
    "\n",
    "Counter = 1\n",
    "for chunk in chunks:\n",
    "    if (hasattr(chunk,'label') and (Counter <=5)):\n",
    "        print(chunk)\n",
    "        Counter = Counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Total Found Named Entity:- 320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Far</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cameron</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Postume Postume</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exactly That</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buckwheat</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Providence</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Lavenham</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Wood</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Wire</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>_To</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entities        Labels\n",
       "0                Far        PERSON\n",
       "1            Cameron        PERSON\n",
       "2    Postume Postume        PERSON\n",
       "3       Exactly That        PERSON\n",
       "4          Buckwheat        PERSON\n",
       "..               ...           ...\n",
       "315       Providence  ORGANIZATION\n",
       "316         Lavenham           GPE\n",
       "317             Wood        PERSON\n",
       "318             Wire        PERSON\n",
       "319              _To  ORGANIZATION\n",
       "\n",
       "[320 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating Data Frame For Named Entity Tagged Words.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "entities =[]\n",
    "labels =[]\n",
    "\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        \n",
    "        entities.append(' '.join(i[0] for i in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "        entities_labels = list(set(zip(entities, labels)))\n",
    "        NER_Data = pd.DataFrame(entities_labels, columns = [\"Entities\",\"Labels\"])\n",
    "        \n",
    "print('##### Total Found Named Entity:-',len(NER_Data))\n",
    "NER_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Pranab Kumar Paul."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
